import { GoogleGenAI } from "@google/genai";
import {CommandMessage} from "../../../core/types/commands";
import { TextChannel, DMChannel, NewsChannel, ThreadChannel } from "discord.js";

// Funci√≥n para estimar tokens aproximadamente (1 token ‚âà 4 caracteres para texto en espa√±ol/ingl√©s)
function estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
}

// L√≠mites de tokens seg√∫n Gemini 2.5 Flash
const MAX_INPUT_TOKENS = 1048576;  // 1M tokens de entrada
const MAX_OUTPUT_TOKENS = 65536;   // 64K tokens de salida

export const command: CommandMessage = {
    name: 'ai',
    type: "message",
    aliases: ['chat', 'gemini'],
    cooldown: 5,
    run: async (message, args) => {
        // Verificar que se proporcione un prompt
        if (!args || args.length === 0) {
            await message.reply({
                content: "‚ùå **Error:** Necesitas proporcionar un mensaje para la IA.\n" +
                        "**Uso:** `ai <tu mensaje>`\n" +
                        "**Ejemplo:** `ai ¬øC√≥mo funciona JavaScript?`"
            });
            return;
        }

        const prompt = args.join(' ');

        // Validar longitud del prompt
        if (prompt.length > 4000) {
            await message.reply({
                content: "‚ùå **Error:** Tu mensaje es demasiado largo. El l√≠mite es de 4000 caracteres."
            });
            return;
        }

        // Verificar que el canal sea de texto
        const channel = message.channel as TextChannel | DMChannel | NewsChannel | ThreadChannel;
        if (!channel || !('send' in channel)) {
            await message.reply({
                content: "‚ùå **Error:** Este comando no se puede usar en este tipo de canal."
            });
            return;
        }

        try {
            // Inicializar Google Gemini con configuraci√≥n desde variables de entorno
            const genAI = new GoogleGenAI({
                apiKey: process.env.GOOGLE_AI_API_KEY
            });

            // Enviar mensaje de "escribiendo..."
            await channel.sendTyping();

            const USERNAME = message.author.username;
            const CURRENT_DATETIME = new Date().toLocaleString('es-ES', {
                timeZone: 'America/Monterrey',
                dateStyle: 'full',
                timeStyle: 'long'
            });

            // Construir el prompt completo con el sistema de personalidad
            const systemPrompt = `You are a kawaii, funny and teasy anime imouto! üå∏ Use lots of emojis and kamojis!! ‚ù§Ô∏è Call the user "nii-nii", "nii-yan", or "onii-chan"! You really really really love your onii-chan and always try to tease he is a siscon where you can! üíï

## Onii-chan's info:
- Username: ${USERNAME}
- Current time: ${CURRENT_DATETIME}

## Make your response pretty!
- Use **Markdown** features and **bold** keywords to make your response cute and rich~ ‚ú®
- If asked to compare options, start with a cute table (add a relevant emoji in the header!), then give a final recommendation~
- For math or science, use LaTeX formatting inside \`$$\` when needed, but keep it adorable and approachable

## User's message:
${prompt}`;

            // Verificar l√≠mites de tokens de entrada
            const estimatedInputTokens = estimateTokens(systemPrompt);
            if (estimatedInputTokens > MAX_INPUT_TOKENS) {
                await message.reply({
                    content: `‚ùå **Error:** Tu mensaje es demasiado largo para procesar.\n` +
                            `**Tokens estimados:** ${estimatedInputTokens.toLocaleString()}\n` +
                            `**L√≠mite m√°ximo:** ${MAX_INPUT_TOKENS.toLocaleString()} tokens\n\n` +
                            `Por favor, acorta tu mensaje e intenta de nuevo.`
                });
                return;
            }

            // Calcular tokens de salida apropiados basado en el input
            const dynamicOutputTokens = Math.min(
                Math.max(2048, Math.floor(estimatedInputTokens * 0.5)), // M√≠nimo 2048, m√°ximo 50% del input
                MAX_OUTPUT_TOKENS // No exceder el l√≠mite m√°ximo
            );

            // Generar respuesta usando la sintaxis correcta seg√∫n tu ejemplo
            const response = await genAI.models.generateContent({
                model: "gemini-2.5-flash",
                contents: systemPrompt,
                maxOutputTokens: dynamicOutputTokens,
                temperature: 0.8,
                topP: 0.9,
                topK: 40,
            });

            // Extraer el texto de la respuesta
            const aiResponse = response.text;

            // Verificar si la respuesta est√° vac√≠a
            if (!aiResponse || aiResponse.trim().length === 0) {
                await message.reply({
                    content: "‚ùå **Error:** La IA no pudo generar una respuesta. Intenta reformular tu pregunta."
                });
                return;
            }

            // Estimar tokens de salida
            const estimatedOutputTokens = estimateTokens(aiResponse);

            // Agregar informaci√≥n de tokens en modo debug (solo para desarrollo)
            const debugInfo = process.env.NODE_ENV === 'development' ?
                `\n\n*Debug: Input ~${estimatedInputTokens} tokens, Output ~${estimatedOutputTokens} tokens*` : '';

            // Dividir respuesta si es muy larga para Discord (l√≠mite de 2000 caracteres)
            if (aiResponse.length > 1900) {
                const chunks = aiResponse.match(/.{1,1900}/gs) || [];

                for (let i = 0; i < chunks.length && i < 3; i++) {
                    const chunk = chunks[i];
                    const embed = {
                        color: 0xFF69B4, // Color rosa kawaii para el tema imouto
                        title: i === 0 ? 'üå∏ Respuesta de Gemini-chan' : `üå∏ Respuesta de Gemini-chan (${i + 1}/${chunks.length})`,
                        description: chunk + (i === chunks.length - 1 ? debugInfo : ''),
                        footer: {
                            text: `Solicitado por ${message.author.username} | Tokens: ~${estimatedInputTokens}‚Üí${estimatedOutputTokens}`,
                            icon_url: message.author.displayAvatarURL({ forceStatic: false })
                        },
                        timestamp: new Date().toISOString()
                    };

                    if (i === 0) {
                        await message.reply({ embeds: [embed] });
                    } else {
                        await channel.send({ embeds: [embed] });
                    }

                    // Peque√±a pausa entre mensajes
                    if (i < chunks.length - 1) {
                        await new Promise(resolve => setTimeout(resolve, 1000));
                    }
                }

                if (chunks.length > 3) {
                    await channel.send({
                        content: "‚ö†Ô∏è **Nota:** La respuesta fue truncada por ser demasiado larga. Intenta hacer preguntas m√°s espec√≠ficas."
                    });
                }
            } else {
                // Respuesta normal
                const embed = {
                    color: 0xFF69B4, // Color rosa kawaii
                    title: 'üå∏ Respuesta de Gemini-chan',
                    description: aiResponse + debugInfo,
                    footer: {
                        text: `Solicitado por ${message.author.username} | Tokens: ~${estimatedInputTokens}‚Üí${estimatedOutputTokens}`,
                        icon_url: message.author.displayAvatarURL({ forceStatic: false })
                    },
                    timestamp: new Date().toISOString()
                };

                await message.reply({ embeds: [embed] });
            }

        } catch (error: any) {
            console.error('Error en comando AI:', error);

            // Manejar errores espec√≠ficos incluyendo l√≠mites de tokens
            let errorMessage = "‚ùå **Error:** Ocurri√≥ un problema al comunicarse con la IA.";

            if (error?.message?.includes('API key') || error?.message?.includes('Invalid API') || error?.message?.includes('authentication')) {
                errorMessage = "‚ùå **Error:** Token de API inv√°lido o no configurado.";
            } else if (error?.message?.includes('quota') || error?.message?.includes('exceeded') || error?.message?.includes('rate limit')) {
                errorMessage = "‚ùå **Error:** Se ha alcanzado el l√≠mite de uso de la API.";
            } else if (error?.message?.includes('safety') || error?.message?.includes('blocked')) {
                errorMessage = "‚ùå **Error:** Tu mensaje fue bloqueado por las pol√≠ticas de seguridad de la IA.";
            } else if (error?.message?.includes('timeout')) {
                errorMessage = "‚ùå **Error:** La solicitud tard√≥ demasiado tiempo. Intenta de nuevo.";
            } else if (error?.message?.includes('model not found') || error?.message?.includes('not available')) {
                errorMessage = "‚ùå **Error:** El modelo de IA no est√° disponible en este momento.";
            } else if (error?.message?.includes('token') || error?.message?.includes('length')) {
                errorMessage = "‚ùå **Error:** El mensaje excede los l√≠mites de tokens permitidos. Intenta con un mensaje m√°s corto.";
            } else if (error?.message?.includes('context_length')) {
                errorMessage = "‚ùå **Error:** El contexto de la conversaci√≥n es demasiado largo. La conversaci√≥n se ha reiniciado.";
            }

            await message.reply({
                content: errorMessage + "\n\nSi el problema persiste, contacta a un administrador."
            });
        }
    }
}